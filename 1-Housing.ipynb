{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67f5ad5-0724-4eac-8fe2-2c916f7f3486",
   "metadata": {},
   "source": [
    "## 1.Importing Data To .CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5991b-44ef-45b2-9e32-2b41d583a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    " os.makedirs(housing_path, exist_ok=True)\n",
    " tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    " urllib.request.urlretrieve(housing_url, tgz_path)\n",
    " housing_tgz = tarfile.open(tgz_path)\n",
    " housing_tgz.extractall(path=housing_path)\n",
    " housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2a555-6a90-429b-9d8d-2a8d405af7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    " csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    " return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ed030-f21a-4270-b40f-8a67680c2b0d",
   "metadata": {},
   "source": [
    "## 2.Analyzing the data\n",
    "let's first import our data and have a quick look at the first 5 columns using head method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8e1c4-7a5b-4461-8ed4-9e62532efc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data() #fetching data\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710fb1e-91b3-488c-9009-c6aa201e85ef",
   "metadata": {},
   "source": [
    "Now let's see infos about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ba7cc-be98-4e43-b222-4556df2d1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafd968-0b71-4c1d-ab9c-b67216bc785d",
   "metadata": {},
   "source": [
    "We remarque that some columns contain non numerical values such as ocean_proximity , let's have a look at how many values does this column may contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68661e2-08d3-461f-8d0d-56a892c3ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d6bdd-ae57-4f8e-a9ac-e015c359a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c48771-d105-4cbe-aa33-b5e868fb6ea0",
   "metadata": {},
   "source": [
    "Another way to analyze the data is by plotting into histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161c012-cbac-4184-961b-783cd5f6a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27211c27-d312-4720-9190-eae0ac4c2e08",
   "metadata": {},
   "source": [
    "## 3.Creating The test set \n",
    "Now we need to create a test set , so we will build a function that splits our data into a train set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dd6e7-97af-4150-8628-2334347120b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_train_test(data, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720cb16-472b-4fee-8b9b-df657c4a4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(len(train_set), \"train +\", len(test_set), \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bfcd8-c192-4ce4-bcde-09736305720a",
   "metadata": {},
   "source": [
    "But both this solution will break next time you fetch an updated dataset ( the one using seed and the one that doesn't )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79843-9520-40ff-866c-1054830e6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf793350-2ec4-4f97-b47e-3dc90fb498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    " bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    " labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2604c-2c17-4b12-924a-a816bee93a0e",
   "metadata": {},
   "source": [
    "Now let's use Startified sampling from scikit learn , basically strata means spliting your dataset into many "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c6773-27cc-41a0-ad63-c81238c16cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    " strat_train_set = housing.loc[train_index]\n",
    " strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a2e6d-adeb-4bf8-acec-4b3c53b80662",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8eb45c-171b-4654-91ca-b5b63ff18084",
   "metadata": {},
   "source": [
    "Now we should remove the income_cat attribute so the data is back to its original\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9d62a-a458-4de6-8953-eba72ad30ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    " set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2b9ae-1187-4d4b-a2d6-aaed6ad9e4da",
   "metadata": {},
   "source": [
    "## 4.Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c8099-a66e-4571-bacd-e231bf36f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e497f-d58a-4017-b8c6-ce2ea6c9f278",
   "metadata": {},
   "source": [
    "Since there is A geo info we can use latitude and longtitude to plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b125e9e-da8d-4e0b-817d-8acf4f27baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64e9ef-24a1-452e-9655-3136f6ccb5a8",
   "metadata": {},
   "source": [
    "Let's set alpha to 0.1 to visualize better the density of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579de6c4-62da-4950-b2b2-16424ff68ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\",x=\"longitude\", y=\"latitude\",alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d1a731-ea43-4c53-88d0-74a086742c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    " s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    " c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda083c-8d60-4363-b9b7-b1595a812f22",
   "metadata": {},
   "source": [
    "## 5.Looking for Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c87122-3760-433b-9bd7-9e35572cd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083b9cb-3551-4fbb-8c3a-1cec6b44953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f39f5-6ea0-4f0b-9f44-d1d60fbbe988",
   "metadata": {},
   "source": [
    "If correlation is close to 1 , it means if a y goes up the, x also goes up , -1 is the opposite and for 0 there is no linear relation between y and x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf28fb-8bb3-426c-a975-17d6d7ccf9fa",
   "metadata": {},
   "source": [
    "Another way to check for correlation between attributes is to use the pandas\n",
    "scatter_matrix() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98563b-95e0-462e-8567-1f6e2d754751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    " \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027d861-3b7d-4754-bb3c-16cb95f497de",
   "metadata": {},
   "source": [
    "The most promising attribute to predict the median house value is the median\n",
    "income, so let’s zoom in on their correlation scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecebdc-83b5-489a-a910-0484081dbb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    " alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20204ec-9213-4423-8af0-99d869e75390",
   "metadata": {},
   "source": [
    "This plot reveals a few things. First, the correlation is indeed very strong; you can\n",
    "clearly see the upward trend, and the points are not too dispersed. Second, the price\n",
    "cap that we noticed earlier is clearly visible as a horizontal line a $500,000. But this\n",
    "plot reveals other less obvious straight lines: a horizontal line aroud $450,000,\n",
    "another aro3d $350,000, perhaps one around $280,000, and a few more below that.\n",
    "You may want to try removing the corresponding districts to prevent your algorithms\n",
    "from learning to reproduce these data quirks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb1c83-e3c0-43a5-ba0f-06c7f602ed12",
   "metadata": {},
   "source": [
    "Since Now we analyzed the data , we may create some attributes that may be more interesting for our case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f26b0-0b3e-49cb-a3eb-1f627b020a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0dc60-4e7c-468c-9e64-a7609baa4474",
   "metadata": {},
   "source": [
    "Now let's see their correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7659efb-82fc-4260-8c16-5f188fa60e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    " corr_matrix = housing.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d9373-2d65-4bf8-93df-b649da28440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a1fc0-8e9a-48b1-a9ec-67bcbfe101df",
   "metadata": {},
   "source": [
    "Hey, not bad! The new bedrooms_per_room attribute is much more correlated with\n",
    "the median house value than the total number of rooms or bedrooms. Apparently\n",
    "houses with a lower bedroom/room ratio tend to be more expensive. The number of\n",
    "rooms per household is also more informative than the total number of rooms in a\n",
    "district—obviously the larger the houses, the more expensive they ar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abf14f-140a-450d-9688-ca6811cb2afa",
   "metadata": {},
   "source": [
    "## Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf41a1-97c2-40f0-98ef-362e6591e68b",
   "metadata": {},
   "source": [
    "Let’s also separate the predictors and the labels, since we don’t necessarily want to\n",
    "apply the same transformations to the predictors and the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb81438-58b2-4925-815b-b16b8b497c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6cda9-1d90-4e15-9c95-3c61f5e59443",
   "metadata": {},
   "source": [
    "Let's replace missing values by the median of that column , that's what we will do with $ total bedrooms $ feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019af19-d2dc-4484-8437-4aaf5f016868",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777addcf-9ccc-477c-9245-d33dc535f2a0",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a handy class to take care of missing values: SimpleImputer.\n",
    "Here is how to use it. First, you need to create a SimpleImputer instance, specifying\n",
    "that you want to replace each attribute’s missing values with the median of that\n",
    "attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68c65e-9779-426b-beb5-5485e2e25676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d2674-9fb4-4a66-a7eb-fb5fb2506559",
   "metadata": {},
   "source": [
    "Since the median can only be computed on numerical attributes, you need to create a\n",
    "copy of the data without the text attribute ocean_proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc607b-3ed0-4031-8054-0bb6827a6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750eb08-0481-4f81-a01e-e57a37f8bc66",
   "metadata": {},
   "source": [
    "Now you can fit the imputer instance to the training data using the fit() method and the result will be stored in statistics method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084075d-95b4-4284-9a50-14c517309bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cb162-a907-462a-9113-39855fcbca62",
   "metadata": {},
   "source": [
    "The imputer has simply computed the median of each attribute and stored the result\n",
    "in its statistics_ instance variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc8051-dcc2-427b-972b-17c1e6f3ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c5aa5-7f5c-423c-b1ad-ffefbfce2104",
   "metadata": {},
   "outputs": [],
   "source": [
    " housing_num.median().values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c998d-a8e1-4b03-b44f-3101abf77575",
   "metadata": {},
   "source": [
    "Now you can use this “trained” imputer to transform the training set by replacing\n",
    "missing values with the learned medians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509d7fa-e9b5-4bcc-ba0c-c97d4f58a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d14013-d759-4479-8049-99b02c70b49b",
   "metadata": {},
   "source": [
    "The result is a plain NumPy array containing the transformed features. If you want to\n",
    "put it back into a pandas DataFrame, it’s simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77179ca-33bd-4a6c-9ebb-d9fcf3b05300",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    " index=housing_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291a476-d090-448f-977c-f7dca36fba95",
   "metadata": {},
   "source": [
    "# Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2205dc-a794-4d57-a4a5-4747f6314ea6",
   "metadata": {},
   "source": [
    "Let's visualize some of the ocean_proximity column containing categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c88d2d-3a14-4973-8abb-b17c3a927f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d06dd-ba70-4c9f-be62-11dfffde5164",
   "metadata": {},
   "source": [
    "Let's use ordinal encoding to convert text values to numerical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7925b1-358f-49a4-81f8-29c099740f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1f2c3-3529-4681-8e20-39586faa6aba",
   "metadata": {},
   "source": [
    "You can get the list of categories using the categories_ instance variable. It is a list\n",
    "containing a 1D array of categories for each categorical attribute (in this case, a list\n",
    "containing a single array since there is just one categorical attribute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ade2b-effd-4676-9c8a-31497f0127f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582c94b-24de-4fdd-876a-1b388bcf4e23",
   "metadata": {},
   "source": [
    "One issue with this representation is that ML algorithms will assume that two nearby\n",
    "values are more similar than two distant values. This may be fine in some cases (e.g.,\n",
    "for ordered categories such as “bad,” “average,” “good,” and “excellent”), but it is obvi‐\n",
    "ously not the case for the ocean_proximity column (for example, categories 0 and 4\n",
    "are clearly more similar than categories 0 and 1). To fix this issue, a common solution\n",
    "is to create one binary attribute per category: one attribute equal to 1 when the cate‐\n",
    "gory is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the cate‐\n",
    "gory is “INLAND” (and 0 otherwise), and so on. This is called ** one-hot encoding, **\n",
    "because only one attribute will be equal to 1 (hot), while the others will be 0 (cold).\n",
    "The new attributes are sometimes called dummy attributes. Scikit-Learn provides a\n",
    "OneHotEncoder class to convert categorical values into one-hot vectors:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd5c85-15a3-496d-8521-ebcce129e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540be881-3f8f-44b1-a2d2-3b2d08d6088e",
   "metadata": {},
   "source": [
    "## Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e261bc7-95a1-44dd-96b4-988f75c97290",
   "metadata": {},
   "source": [
    "Let's create a custom Transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9329c7e-5f52-4df4-a25c-d9e59085f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = [housing.columns.get_loc(c) for c in col_names] ## dynamic name extraction\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs , hyperparameters\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "housing.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bcbca-b1d3-4096-9331-5c54c12a7282",
   "metadata": {},
   "source": [
    "The previous code create a transformer that create different new features using the BaseEstimator and TransformMixin functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c4ac5-765e-43ae-a012-0f45d292bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_extra_attribs = pd.DataFrame(\n",
    "    housing_extra_attribs,\n",
    "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n",
    "    index=housing.index)\n",
    "housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c9755-e76f-42f2-9041-7189fd36e966",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed2107-a0b4-4e3c-bd17-51081f03b402",
   "metadata": {},
   "source": [
    "There are two common ways to get all attributes to have the same scale: min-max (normalization)\n",
    "scaling and standardization. for min max , it's simple we substract the min value and divide by the max value all of our example. Whereas standarization first subtracts the mean value (so standardized values \n",
    "always have a zero mean), and then it divides by the standard deviation so that th \n",
    "resulting distribution has unit varian.ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65856f4f-71ec-4e8b-b2fe-ba041f867205",
   "metadata": {},
   "source": [
    "## Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca773da8-3087-4b41-bde6-7d3a8a9066b8",
   "metadata": {},
   "source": [
    "Pipelines in scikitlearn help us gather all the transofmation we want to apply to the data in the same order . SO let's create a transformation pipeline for our numerical features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fdd42-bc1e-489a-aed0-1c1505d503f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline = Pipeline([\n",
    " ('imputer', SimpleImputer(strategy=\"median\")),\n",
    " ('attribs_adder', CombinedAttributesAdder()),\n",
    " ('std_scaler', StandardScaler()),\n",
    " ])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2a2a6-6735-4e2c-8bc3-07a964f8adc9",
   "metadata": {},
   "source": [
    "There is a way to both treat numerical and categorical data in one shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451283c-7947-40ad-a4e4-629377a194de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    " (\"num\", num_pipeline, num_attribs),\n",
    " (\"cat\", OneHotEncoder(), cat_attribs),\n",
    " ])\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0d407-7463-44be-899e-652329653b77",
   "metadata": {},
   "source": [
    "## Select and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca8f3c-a37c-417d-94b6-a058b143235d",
   "metadata": {},
   "source": [
    "now it's time to select a model to train it on our cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d531cb-1f67-4aa0-8af0-763514775520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789a895-6ea5-4f0c-9596-bf5d1f9ba8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18634605-ed5f-4a2c-9048-a2319c1f8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b67dd-3eb0-4572-9501-7cd3ff06fd4e",
   "metadata": {},
   "source": [
    "It works, although the predictions are not exactly accurate (e.g., the first prediction is\n",
    "off by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\n",
    "ing set using Scikit-Learn’s mean_squared_error() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216f725-d32a-4eef-8d75-2a213d25c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607a809-1d28-422c-885d-a718547ff9da",
   "metadata": {},
   "source": [
    "Now let' train it using A decisionTree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f8a38-9c34-4eed-96bc-328a6d3f2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ad245-8f33-4093-8105-03f41bb9d163",
   "metadata": {},
   "source": [
    "let's evaluate it using RMSE this time again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5b950-1bca-4f5c-b6aa-052db067f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions= tree_reg.predict(housing_prepared)\n",
    "tree_mse= mean_squared_error(housing_labels,housing_predictions)\n",
    "tree_rmse=np.sqrt(tree_mse)\n",
    "tree_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c04be-94ac-48a4-8052-2a0a5cfb657c",
   "metadata": {},
   "source": [
    "Is this model perfect or WHAT !? no it's not , it just overfits . As we mentioned previously we should't touch the test set while training the model. next we will tackle cross validation method !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df259775-0631-44d7-83d1-eb8ddc593a17",
   "metadata": {},
   "source": [
    "## Better Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c02f68-8a1e-4b6e-8d46-0a6cd8f69f36",
   "metadata": {},
   "source": [
    "let's use K-fold cross validation method to test which fold of the data work better for test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09da61-87b1-436b-b7a2-27fd24226ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    " scoring=\"neg_mean_squared_error\", cv=10)## we'll be using 10 folds\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73c4c8-b6f0-44e9-96e6-2577e0e85ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c20250-b5fa-472c-963d-e6c239b4421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9865d-fd13-4046-9848-9ee5946bf35a",
   "metadata": {},
   "source": [
    "Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to perform worse than the Linear Regression model! but this method needs a lot of computational power , so it's not always availble to try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd60041-df5c-466d-92e1-25f0a0c27057",
   "metadata": {},
   "source": [
    "Now let's try doing cross validation for tghe linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712231b-eda9-4c7c-a49f-c694ef5e7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "lin_rmse_scores=np.sqrt(-lin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55280d1e-bcfb-43f3-96aa-a148b49eba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d033ff-6906-4dda-a0a8-3ba770636179",
   "metadata": {},
   "source": [
    "let's try now the same thing using random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f475ab-d547-423c-ad9d-ef2060ee0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a92b94-13a4-4812-b1df-b6e2d584edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions= forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7b8dd-717c-481e-b9fd-7a3286c04d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores = cross_val_score(forest_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "forest_rmse_scores=np.sqrt(-forest_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a4f46-fe7c-47e7-9bae-3d1c912803fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192daa49-f354-4e27-93eb-84231453ac98",
   "metadata": {},
   "source": [
    "## Fine-Tune The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d962ba-1ca6-4f72-bd05-b3ee25974c2f",
   "metadata": {},
   "source": [
    "Let's use GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af020196-97d3-4d4c-85f9-772e941cd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    " {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    " {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    " ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    " scoring='neg_mean_squared_error',\n",
    "return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb7696-9fc3-43df-82fd-a5f5d0a077a5",
   "metadata": {},
   "source": [
    "this may take a long time , becasueit will explore all the available combinations and apply cross validation to them. We will find the result in grid_search.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a88ddbe-a010-4c08-9761-afdecbfbbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b429bfb-8d10-4da3-9cec-98af04e00030",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbd56a-1b43-44fc-ae1e-b1df7e387d77",
   "metadata": {},
   "source": [
    "Here we can find the scores for each attribute ( column )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f7857-6f27-4ec9-b558-f66a4af589f6",
   "metadata": {},
   "outputs": [],
   "source": [
    " cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5eabc-33c3-411f-a986-5cb8c098bd83",
   "metadata": {},
   "source": [
    "## Randomized Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db335492-0d70-4f1c-9502-393febe2122d",
   "metadata": {},
   "source": [
    "The grid search approach is fine when you are exploring relatively few combinations,\n",
    "like in the previous example, but when the hyperparameter search space is large, it is\n",
    "often preferable to use RandomizedSearchCV instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bf256-d488-4428-aa47-47ba8b4fe2b0",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e5b12-c952-459b-89d5-e39de438c903",
   "metadata": {},
   "source": [
    "By combining the models that performs the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595202b-e21d-4d89-8dfb-691de504ded5",
   "metadata": {},
   "source": [
    "## Analyze the Best Models and Their Errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22644b8d-c80d-4e6b-85d4-a349e57c1a01",
   "metadata": {},
   "source": [
    "The RandomForestRegressor can indicate the relative importance of each\n",
    "attribute for making accurate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc985f8-c4c7-4aae-9f39-6802e7e4802b",
   "metadata": {},
   "outputs": [],
   "source": [
    " feature_importances = grid_search.best_estimator_.feature_importances_\n",
    " feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf24ce-476e-44ab-a2a7-5abaebb8ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    ">>> cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    ">>> cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    ">>> attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    ">>> sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd20605-fcfd-432c-87cd-e65547d0799c",
   "metadata": {},
   "source": [
    "## Evaluate Your System on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b8318-bb12-4525-a4d8-7f016cc45ae2",
   "metadata": {},
   "source": [
    "let's finally evaluate our model using the RMSE error estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306a27d-25a5-4a2b-9f21-d9ad38d7cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aedfa4-e32f-4a97-b114-a2a0557c2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) \n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006640f6-5a60-45ba-a1ba-2de440e572dd",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b4bc2-7fc9-4a40-8706-357255fb719e",
   "metadata": {},
   "source": [
    "### 1- Using SVM model on the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee845a14-ba1e-4c71-aedf-97a0a28c26df",
   "metadata": {},
   "source": [
    "Using the SVR estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac12d5-13b3-4c3a-8fb6-d866bb60938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "SVR_reg= SVR()\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "svm_grid_search = GridSearchCV(SVR_reg,param_grid, cv=5 ,scoring=\"neg_mean_squared_error\",return_train_score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
